---
title: "crossvalidation"
author: "Xuehan Yang"
date: "2021/11/18"
output: github_document
---

Although hypothesis tests provide a way to compare nested linear models, in many situations the approaches under consideration donâ€™t fit nicely in this paradigm. Indeed, for many modern tools and in many applications, the emphasis lies on prediction accuracy rather than on statistical significance. In these cases, cross validation provides a way to compare the predictive performance of competing methods.

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(modelr)
library(mgcv)
library(viridis)



knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## CV "by hand", Simulate a dataset

```{r, message=FALSE, warning=FALSE}
set.seed(1)

nonlin_df = tibble(
  id = 1:100,
  x  = runif(100, 0, 1),
  y  = 1 - 10*(x - .3)^2 + rnorm(100, 0, .3)  
)

nonlin_df %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point()
```

Create splits by hand, plot, fit models

```{r,message=FALSE, warning=FALSE}
train_df = sample_n(nonlin_df, 80)
test_df = anti_join(nonlin_df, train_df, by = "id")

ggplot(train_df, aes(x = x, y = y)) +
  geom_point() +
  geom_point(data = test_df, color = "red")

```

Fit my models.

```{r, message=FALSE, warning=FALSE}
linear_mod = lm(y ~ x, data = train_df)
smooth_mod = mgcv::gam(y ~ s(x), data = train_df)
wiggly_mod = mgcv::gam(y ~ s(x, k = 30), sp = 10e-6, data = train_df)
```

Plot the results

```{r,message=FALSE, warning=FALSE}
train_df %>% 
  add_predictions(smooth_mod) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(y = pred))
```

```{r, message=FALSE, warning=FALSE}
train_df %>% 
  add_predictions(wiggly_mod) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(y = pred))
```

```{r, message=FALSE, warning=FALSE}
train_df %>% 
  add_predictions(linear_mod) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(y = pred))
```

quantify the results

```{r, message=FALSE, warning=FALSE}
rmse(linear_mod, test_df)
rmse(smooth_mod, test_df)
rmse(wiggly_mod, test_df)
```

## CV iteratively

USe `modelr::crossv_mc`

```{r, message=FALSE, warning=FALSE}
cv_df = 
  crossv_mc(nonlin_df, 100) # 100 trainings and tests

cv_df %>% pull(train) %>% .[[1]] %>% as_tibble()
```

```{r, message=FALSE, warning=FALSE}
cv_df = 
  crossv_mc(nonlin_df, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  )
```

lets fit models

```{r, message=FALSE, warning=FALSE}
cv_df = 
cv_df %>% 
  mutate(
    linear_mod = map(.x = train, ~lm(y ~ x, data = .x)),
    smooth_mod = map(.x = train, ~gam(y ~ s(x), data = .x)),
    wiggly_mod = map(.x = train, ~gam(y ~ s(x, k = 30), sp = 10e-6, data = .x))
    ) %>% 
  mutate(
    rmse_linear = map2_dbl(.x = linear_mod, .y = test, ~rmse(data = .y, model = .x)),
    rmse_smooth = map2_dbl(.x = smooth_mod, .y = test, ~rmse(data = .y, model = .x)),
    rmse_wiggly = map2_dbl(.x = wiggly_mod, .y = test, ~rmse(data = .y, model = .x))
  )
```

Look at output
```{r, message=FALSE, warning=FALSE}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    rmse_linear:rmse_wiggly,
    names_to = "model",
    values_to = "rmse",
    names_prefix = "rmse_"
  ) %>% 
  ggplot(aes(x = model, y = rmse)) +
  geom_boxplot()
```

## Child growth data

import data
```{r, message=FALSE, warning=FALSE}
child_growth_df = read_csv("cross_validation_files/nepalese_children.csv") %>% 
  mutate(
    weight_cp = (weight > 7)*(weight - 7)
  )
```

```{r, message=FALSE, warning=FALSE}
child_growth_df %>% 
  ggplot(aes(x = weight, y = armc)) +
  geom_point(alpha = .2)
```

consider candidate models.

```{r, message=FALSE, warning=FALSE}
linear_mod = lm(armc ~ weight, data = child_growth_df)
pwl_mod    = lm(armc ~ weight + weight_cp, data = child_growth_df)
smooth_mod = gam(armc ~ s(weight), data = child_growth_df)
```

```{r, message=FALSE, warning=FALSE}
child_growth_df %>% 
  add_predictions(linear_mod) %>% 
  ggplot(aes(x = weight, y = armc)) +
  geom_point(alpha = .2) +
  geom_line(aes(y = pred), color = "red")
```

Use cv to compare models

```{r, message=FALSE, warning=FALSE}
cv_df = 
  crossv_mc(child_growth_df, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  )
```

Fit models and extract rmse

```{r, message=FALSE, warning=FALSE}
cv_df = 
cv_df %>% 
  mutate(
    linear_mod = map(.x = train, ~lm(armc ~ weight, data = .x)),
    pwl_mod    = map(.x = train, ~lm(armc ~ weight + weight_cp, data = .x)),
    smooth_mod = map(.x = train, ~gam(armc ~ s(weight), data = .x))
    ) %>% 
  mutate(
    rmse_linear = map2_dbl(.x = linear_mod, .y = test, ~rmse(data = .y, model = .x)),
    rmse_smooth = map2_dbl(.x = smooth_mod, .y = test, ~rmse(data = .y, model = .x)),
    rmse_pwl = map2_dbl(.x = pwl_mod, .y = test, ~rmse(data = .y, model = .x))
  )
```

Look at RMSE distribution

```{r, message=FALSE, warning=FALSE}
cv_df %>% 
  select(.id, starts_with("rmse")) %>% 
  pivot_longer(
    rmse_linear:rmse_pwl,
    names_to = "model",
    values_to = "rmse",
    names_prefix = "rmse_"
  ) %>% 
  ggplot(aes(x = model, y = rmse)) +
  geom_boxplot()
```











